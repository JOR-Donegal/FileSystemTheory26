{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>File System Theory</p> <p>The first computers I used loaded programmes and data directly off magnetic tape. As the data was laid out in sequence from the start of the tape to the end, we referred to tape solutions as linear. And linear solutions are still used for backup and archiving purposes.</p> <p>Once floppy disks became available, we moved to file systems.</p> <p>And once hard disks became available, we moved to complex filesystems with features to enhance security, availability and durability or the data.</p>"},{"location":"1.%20MS-DOS/a/","title":"DOS and FAT","text":"<p>When the PC first emerged, it did so with the File Allocation Table or FAT file system. Disks were divided into sectors of 512 bytes and this is the smallest block of data which could be stored. The file system had a fixed format, where the first sector of the file system contained meta-data about the whole disk. If the disk was bootable, the first three bytes contained a machine code jump instruction, pointing at the operating system\u2019s boot loader. </p> <p>The FAT itself was a table of pointers to sectors which contained file data and a separate table existed called a directory, with the names of the files. Content was stored in clusters</p> <ul> <li>Sectors were 512 bytes </li> <li>Cluster = 2 or more * consecutive sectors </li> <li>The first cluster address is cluster 2 </li> <li>If file &gt; 1 cluster, locations found using File Allocation Table (FAT)</li> </ul> Fig 1. FAT partition layout <p>When FAT12 emerged in 1980 it was used on single sided, single density disks with a typical capacity of 180KB. The naming was the crude 8.3 format where a filename was something like JOHNORAW.TXT and no security or ownership information lived in the file system.</p> <p>For a worked example demonstrating FAT, take a look at this video from 2019.</p> <p>The early hard drives which appeared with PC technology quickly outstripped the capabilities of the operating systems we had at the time, and of the file systems they implemented to keep track of where files were saved. The maximum capacity of FAT was 512 * 216 or 32MB, so when I got my first 120MB hard drive, I had a problem! There was no way to format it so that I could use more than 32MB, the file system just could not map it.</p> <p>In 1983 the concept of a partition was introduced by IBM to PCs with version 2 of PC-DOS. Partitions allowed us to break up a hard drive into separate logical areas, each of which could contain its own file system. To the operating system, it appeared as if it had more than one hard drive, each with its own separate file system. </p>"},{"location":"2.%20Windows%20NT/b/","title":"NTFS","text":"<p>In the early 1990s, Microsoft had realised that their original DOS/Windows technology had reached the end of feasible development and that they needed new beginnings. The Windows New Technoloy (NT) project was successful and with it came a new file system to replace FAT. FAT dated back to the 1970 and was archaic and obsolete as a server file system; it remains to this day as a suitable file system for removable media. </p> <p>HPFS was designed for use with OS/2 and was better, but suited for desktop, not server environments. </p> <p>Microsoft learned a great deal from the development of HPFS and these lessons were implemented in their new file system (NTFS).</p> <p>NTFS was written by a team headed by Tom Miller.</p> <p>It was built to be:</p> <ul> <li>Reliable, with metadata redundancy and fault tolerance.</li> <li>Secure.</li> <li>Recoverable using transaction processing, journaling.</li> </ul> <p>It was intended to be scalable to cope with large volumes, partitions, disks, and files.</p> <ul> <li>2<sup>64</sup> files of 2<sup>64</sup> bytes long</li> <li>On Windows Server 2012 the maximum file size was 256TB</li> <li>2<sup>64-1</sup> Clusters</li> <li>With a standard cluster size of 4KB, a volume can be just under 16TB.</li> </ul> <p>NTFS was not designed for removable media as it uses a lazy write scheme.  The cache manager deliberately buffers and delays writing data to disk to boost performance. Grouping many small writes into fewer big writes is much faster. NTFS journals metadata, not file contents, after a crash the file system structure is consistent but recent file data may be missing or corrupt.</p> <p>The file system is structured as a database and uses the concept of transactions in the form of a journal.</p> <ul> <li>The metadata file $LogFile is used to log changes to metadata.</li> <li>Changes to files and directories are recorded in the Update Sequence Number Journal (USN).</li> </ul> <p>As with the Apple file systems of the time, there are multiple data streams, each attribute is a stream.</p> <p>Compression was built into the original standard and encrypted directories (EFS) were added later. Sparse files can be accommodated; these are files with empty space within them. Then empty space is recorded but does not take up storage space.</p> <p>NTFS support Volume Shadow Copy (VSC) to allow earlier versions of files to be recovered. This is useful for recovering files and used by file system backups to access files which are in use and locked.</p> <p>Transactional NTFS allows a transaction involving a group of files to be treated as a single transaction, in the event of the transaction not completing, the entire transaction can be rolled back.</p> <p>Security was introduced. Two sets of Access Control Lists (ACLs) are defined, one for discretionary access control (DAC) and the other for mandatory access control (DAC). Read about DAC and MAC to fully understand, but simplified, with DAC the user decides on permissions. With MAC, system policies determine permissions.</p> <p>Disk Quotas were introduced to limit user\u2019s exploitation of disk space.</p>"},{"location":"2.%20Windows%20NT/c/","title":"Volume Structure","text":"<p>A volume is the largest unit of structure in NTFS. Each file on an NTFS volume is represented by a record in the Master File Table or MFT.</p> <p>A system file is one used by the file system to store its metadata and to implement the file system. System files are placed on the volume by the Format utility. Everything in NTFS is a file.</p> Fig 2. NTFS partition layout"},{"location":"2.%20Windows%20NT/c/#boot","title":"$boot","text":"<p>On an NTFS partition, the first 16 sectors are allocated to the partition boot sectors and the file $boot.</p> <p>The first sector contains meta-data similar to other file systems like FAT, with a BIOS parameter block (BPB) so it really is a boot sector like FAT has.</p> <p>Bootstrap code exists from 0x54.</p> Fig 3. First sector layout."},{"location":"2.%20Windows%20NT/c/#the-bios-parameter-block","title":"The BIOS Parameter Block","text":"<p>The BIOS Parameter Block is similar to the reserved are at the start of a FAT partition. The fields starting at 0x0B, 0x0D, 0x15, 0x18, 0x1A, and 0x1C match those on FAT16 and FAT32 volumes.</p> Fig 4. BPB."},{"location":"2.%20Windows%20NT/c/#extended-bios-parameter-block","title":"Extended BIOS Parameter Block","text":"<p>The Extended BIOS Parameter Block holds additional fields for locating the MFT, the most important metadata structure in NTFS. The MFT is not located in a fixed sector (although it almost always is by default).</p> Fig 5. EBPB."},{"location":"2.%20Windows%20NT/c/#initial-programme-loader-ipl","title":"Initial Programme Loader (IPL)","text":"<p>There are 15 sectors reserved for the Initial Program Loader (IPL) code. 15 * 4,096 = 61,440 bytes for the IPL.</p>"},{"location":"2.%20Windows%20NT/d/","title":"MFT","text":"<p>NTFS keeps track of the contents of a volume using the MFT, which is a relational database. Space is allocated as clusters (2<sup>n</sup> sectors) and all storage uses Logical Cluster Numbers (LCNs). A copy of the MFT is kept elsewhere in the file system; this is the $MFTMirror.</p> <p>The MFT keeps entries for every file in the system. The first 16 entries in the MFT are reserved for system metadata, which is all stored as files.</p> <p>At the very least, each MFT entry must have - A header - Attribute type 16: Standard Information - Attribute type 48: File Name - Attribute type 128: Data</p> Fig 6. An MFT Entry.  <p>NTFS views files as a collection of attributes and directories as a file with slightly different attributes.</p> Fig 7. The MFT.  <p>The rows of the MFT correspond to individual files, the columns correspond to file attributes.</p>"},{"location":"2.%20Windows%20NT/e/","title":"System Files","text":"<p>NTFS reserves the first 16 records of the MFT for system files containing special information about the file system itself. The first record of the MFT describes the master file table itself, followed by a MFT mirror record. If the first MFT record is corrupted, NTFS reads the second record to find the MFT mirror file, whose first record is identical to the first record of the MFT. The locations of the data segments for both the MFT and MFT mirror file are recorded in the boot sector (Offset 0X30 and 0X38).</p> Table 1. System Files"},{"location":"3.%20Linux/a/","title":"The Virtual File System","text":"<p>Before introducing new file system types, an additional layer of abstraction was added to Linux, the Virtual File System or VFS. This was not a new concept and dates back at least to SunOS in the 1980s.</p> <p>This layer allows file system calls to be made to the VFS, rather than to the physical file systems code. These eases migration to new file system types and to mixed file systems on a single system. The VFS defines functions which every file system will implement. File system types are defined in the kernel.</p> <p>When a file system is to be mounted, a function is called which reads the superblock from the file system, initializes variables, and returns a descriptor to the VFS.</p> <p>Linux can deal with a range of local file systems, or with network-based file systems like NFS. In more recent times, code has been implemented for iSCSI and Fibre Channel.</p> Fig 8. The VFS."},{"location":"3.%20Linux/b/","title":"extX File System Types","text":"<p>In 1992, the First Extended File System or Ext was introduced into the Linux kernel v0.96c. This allowed for a file size of 2GB and a file system size of 2GB, with a 255-character file name. The original file system used linked lists to keep track of space and this was inefficient, slow and led to fragmentation.</p> <p>The Second Extended File System or Ext2 was designed as an evolution from Ext and became the standard for Linux systems. It supported partitions of 4TB. It had support for longer names, up to 1,012 characters. Logical block size could be determined at format time; the standard was 1024 but 2048 and 4096 were possible. Ext2 kept track of the state of the file system. When a file system is mounted in read/write mode, the state is set to not clean. When the file system is unmounted, its state is set to clean. At boot time, this indicates that a file system was improperly shut down and must be checked. Clean or not, a file system check also runs periodically after a set number of mounts.</p> <p>Ext3, the Third Extended File System was introduced with journaling to aid file system recovery and also with access control list features (ACLs). Other than that, it was a minor upgrade to Ext2. ext3 uses a journal or log to protect consistency after crashes, implemented via the Journaling Block Device(JBD). Journalling is a similar concept to log files in a database.</p> <ul> <li>Metadata is journaled</li> <li>File data is written to disk before the metadata is committed</li> <li>The journal entry is then cleared</li> </ul> <p>Ext4 has emerged in recent years as the Fourth Extended File System to eliminate the scaling issues in Ext2/3. It introduces many modern concepts, and I would imagine learns from NTFS! The journalling is better.</p> <ul> <li>Transactions are checksummed</li> </ul>"},{"location":"3.%20Linux/c/","title":"iNodes","text":"<p>Almost every UNIX file system uses the same concepts for file system organisation.</p> <p>Every file is represented by a metadata structure called an I-node.  An I-node stores information about a regular file, directory, or other file system object in 128-byte records:</p> <ul> <li>File type (executable, block special etc.)</li> <li>Permissions (read, write etc.)</li> <li>Owner, Group</li> <li>File Size</li> <li>File access, change and modification time, File deletion time.</li> <li>Number of links (soft/hard)</li> <li>Extended attribute such as append only.</li> <li>Access Control List (ACLs)</li> </ul> <p>A directory I-node is a list of files and directories.  Its purpose is to make a link between file names and I-node numbers.  In Ext2/3 there is a limit of 32,768 subdirectories and a limit of 10-15,000 files per directory.</p> Fig 9. An iNode.  <p>I can use ls to find the iNode number of the current and parent directory or of any file. </p> <pre><code>johnoraw@RPi5-DataStore1:~ $ ls -id ./\n14417922 ./\njohnoraw@RPi5-DataStore1:~ $ ls -id ../\n14417921 ../\njohnoraw@RPi5-DataStore1:~ $ ls -id main.py \n14549744 main.py\n</code></pre> Table 2. iNode numbers.  <p>All files and directories can be listed with their names and their I-node numbers. The block locations for each files and directory can then be looked up in the I-Node table. As the filenames are stored in a directory entry and the I-nodes are stored separately, any I-node can have more than one name. This is very useful and gives us the concept of a link, where multiple directory entries point at the same I-node. The I-node in turn has a link counter which keeps track of the number of directory entries pointing at the I-node.</p> <p>When a user deletes a file, this link count is decremented. When the link count reaches zero, the I-node is deallocated. These are called hard links and because they use I-node numbers, they cannot cross a file system boundary (as I-nodes will only be unique per file system).</p> <p>Where we need links to cross system boundaries, we use symbolic links. These are just files which contain a file name. When the kernel encounters a symbolic link, it replaces the link with the contents of the file and continues whatever request it was servicing.</p>"},{"location":"3.%20Linux/d/","title":"Direct Pointers","text":"<p>The address of the data blocks which contain the file data are held in the I-node; there are 15 pointers but only the first 12 can be used as direct pointers.  So in a file system with 4KB blocks, files of up to 48KB can be addressed.</p> Fig 10. Pointers."},{"location":"3.%20Linux/e/","title":"Indirect Pointers","text":"<p>Where files are bigger, we can use the thirteenth pointer, the indirect pointer.  This points at a 4KB block which itself contains pointers.  Block pointers are all 4 bytes quantities, so 1,024 blocks can be addressed in this way.  We can now store file data in 12 + 1,024 blocks, or around 4.2MB.</p> Fig 11. Indirect Pointers."},{"location":"3.%20Linux/f/","title":"Double Indirect Pointers","text":"<p>If the files are larger, the fourteenth block can point to a block, which in turn points to other blocks of pointers; this is a doubly indirect pointer.  We can now address a file size of 12 + 1,024 + (1,024*1,024) blocks, or about 4.3GB.</p> Fig 12. Double indirect Pointers."},{"location":"3.%20Linux/g/","title":"Triple indirect Pointers","text":"<p>Finally, the fifteenth pointer can point to a block, which points to other blocks, which points to other blocks, for a triple indirect pointer.  We can now address a file size of 12 + 1,024 + (1,0241,024) + (1,0241,024*1,024) blocks, or about 4.4TB.</p> Fig 13. Triple indirect Pointers.  <p>In the 1980s when these concepts were first developed, these file sizes were unimaginable.  We now need to work with files greater than this size.  We will look at that when we discuss extents under Ext4.</p>"}]}